social_marketing$cluster <- as.factor(kmeans_result$cluster)
pca <- prcomp(social_marketing_scaled)
pca_data <- data.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], Cluster = social_marketing$cluster)
ggplot(pca_data, aes(x = PC1, y = PC2, color = Cluster)) +
geom_point() +
labs(title = "PCA of Social Marketing Data with K-Means Clusters") +
theme_minimal()
# Scale the data
social_marketing_scaled <- scale(social_marketing)
set.seed(123)  # For reproducibility
# Perform K-Means clustering with a chosen number of clusters (e.g., 4)
kmeans_result <- kmeans(social_marketing_scaled, centers = 4, nstart = 25)
# Add the cluster assignment to the original data
social_marketing$cluster <- as.factor(kmeans_result$cluster)
# Elbow method to determine the number of clusters
wss <- sapply(1:10, function(k) {
kmeans(social_marketing_scaled, k, nstart = 10)$tot.withinss
})
# Elbow method to determine the number of clusters
wss <- sapply(1:10, function(k) {
kmeans(social_marketing_scaled, k, nstart = 10)$tot.withinss
})
plot(1:10, wss, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters K",
ylab = "Total within-clusters sum of squares",
main = "Elbow Method for Optimal K")
# Assuming 2 clusters is the optimal number
kmeans_result <- kmeans(social_marketing_scaled, centers = 2, nstart = 25)
social_marketing$cluster <- as.factor(kmeans_result$cluster)
# Perform PCA for visualization
pca_result <- prcomp(social_marketing_scaled, scale. = TRUE)
# Create a data frame for the PCA results
pca_df <- data.frame(pca_result$x[, 1:2], Cluster = social_marketing$cluster)
# Plot the clusters
ggplot(pca_df, aes(x = PC1, y = PC2, color = Cluster)) +
geom_point(size = 2) +
theme_minimal() +
labs(title = "PCA Plot of Clusters", x = "Principal Component 1", y = "Principal Component 2")
cluster_summary <- social_marketing %>%
group_by(cluster) %>%
summarise_all(list(mean = mean))
print(cluster_summary)
# Plot the clusters
ggplot(pca_df, aes(x = PC1, y = PC2, color = Cluster)) +
geom_point(size = 2) +
theme_minimal() +
labs(title = "PCA Plot of Clusters", x = "Principal Component 1", y = "Principal Component 2")
print(cluster_summary)
View(cluster_summary)
# Load libraries
library(tm)
# Load libraries
library(tm)
library(SnowballC)
library(tidyverse)
library(cluster)
library(factoextra)
library(wordcloud)
train_path <- file.path(main_path, "C50train")
main_path <- "ReutersC50"  # Set this to your folder path
train_path <- file.path(main_path, "C50train")
test_path <- file.path(main_path, "C50test")
# Load the training data
train_corpus <- Corpus(DirSource(train_path, recursive = TRUE),
readerControl = list(reader = readPlain, language = "en"))
View(train_corpus)
# Load the test data (if needed)
test_corpus <- Corpus(DirSource(test_path, recursive = TRUE),
readerControl = list(reader = readPlain, language = "en"))
corpus <- tm_map(corpus, content_transformer(tolower))
preprocess_corpus <- function(corpus) {
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, stemDocument)  # Optional: for stemming
return(corpus)
}
# Preprocess the training data
train_corpus <- preprocess_corpus(train_corpus)
View(train_corpus)
# Preprocess the test data (if needed)
test_corpus <- preprocess_corpus(test_corpus)
# Create a Document-Term Matrix (DTM):
dtm_train <- DocumentTermMatrix(train_corpus)
# Create a Document-Term Matrix (DTM):
dtm_train <- DocumentTermMatrix(train_corpus)
dtm_test <- DocumentTermMatrix(test_corpus)  # If you plan to use the test set
View(dtm_train)
inspect(dtm_train[1:5, 1:10])  # View the first 5 documents and the first 10 terms
# Perform K-means clustering
set.seed(123)
k <- 5
km_res <- kmeans(tfidf, centers = k, nstart = 25)
# tfidf is the weighted document-term matrix where term frequencies are adjusted using the TF-IDF method
# Reduces the influence of common words and highlights distinctive words
tfidf <- weightTfIdf(dtm)
# Create a Document-Term Matrix (DTM):
dtm <- DocumentTermMatrix(train_corpus)
# tfidf is the weighted document-term matrix where term frequencies are adjusted using the TF-IDF method
# Reduces the influence of common words and highlights distinctive words
tfidf <- weightTfIdf(dtm)
# Perform K-means clustering
set.seed(123)
k <- 5
km_res <- kmeans(tfidf, centers = k, nstart = 25)
# Visualize clusters with PCA
pca_res <- prcomp(tfidf, scale = TRUE)
# Visualize clusters with PCA
pca_res <- prcomp(tfidf, scale = TRUE)
fviz_cluster(km_res, data = pca_res$x)
fviz_cluster(km_res, data = pca_res$x)
View(pca_res)
# Perform K-means clustering
set.seed(123)
k <- 5
km_res <- kmeans(pca_res$x[, 1:100], centers = k, nstart = 25)  # Use first 100 PCs
# Visualize clusters
fviz_cluster(km_res, data = pca_res$x[, 1:100])  # Use the same reduced dimensions
# Perform K-means clustering
# Set a range for the number of clusters
k_values <- 1:10
wcss <- numeric(length(k_values))
# Calculate WCSS for each k
for (k in k_values) {
set.seed(123)
kmeans_model <- kmeans(tfidf, centers = k, nstart = 25)
wcss[k] <- kmeans_model$tot.withinss
}
# Step 1: Apply PCA
pca_result <- prcomp(tfidf, scale. = TRUE)
# Choose the number of components to keep (e.g., 95% of variance)
explained_variance <- cumsum(pca_result$sdev^2) / sum(pca_result$sdev^2)
num_components <- which(explained_variance >= 0.95)[1]
# Keep only the top components
pca_data <- pca_result$x[, 1:num_components]
# Step 2: Run K-means on the PCA-transformed data
k_values <- 1:10
wcss <- numeric(length(k_values))
for (k in k_values) {
set.seed(123)
kmeans_model <- kmeans(pca_data, centers = k, nstart = 25)
wcss[k] <- kmeans_model$tot.withinss
}
# Step 3: Plot the Elbow Plot
plot(k_values, wcss, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters K",
ylab = "Total Within-Cluster Sum of Squares (WCSS)",
main = "Elbow Plot after PCA for Determining Optimal Number of Clusters")
# Step 2: Run K-means on the PCA-transformed data
k_values <- 1:10
wcss <- numeric(length(k_values))
for (k in k_values) {
set.seed(123)
kmeans_model <- kmeans(pca_data, centers = k, nstart = 25)
wcss[k] <- kmeans_model$tot.withinss
}
for (k in k_values) {
set.seed(123)
kmeans_model <- kmeans(pca_data, centers = k, nstart = 25)
wcss[k] <- kmeans_model$tot.withinss
}
# Step 3: Calculate the percentage change in WCSS
wcss_diff <- diff(wcss) / wcss[-length(wcss)] * 100
# Step 2: Run K-means on the PCA-transformed data
k_values <- 1:10
wcss <- numeric(length(k_values))
for (k in k_values) {
set.seed(123)
kmeans_model <- kmeans(pca_data, centers = k, nstart = 25)
wcss[k] <- kmeans_model$tot.withinss
}
for (k in k_values) {
set.seed(123)
kmeans_model <- kmeans(pca_data, centers = k, nstart = 25)
wcss[k] <- kmeans_model$tot.withinss
}
# Step 3: Calculate the "elbow" point
# Calculate the differences between successive WCSS values
wcss_diff <- diff(wcss)
# Identify the elbow point as the first major drop
elbow_k <- which.max(wcss_diff) + 1
# Print the optimal number of clusters
cat("The optimal number of clusters based on the elbow method is:", elbow_k, "\n")
# Optional: Visualize the Elbow Plot
plot(k_values, wcss, type = "b", pch = 19, frame = FALSE,
xlab = "Number of Clusters K",
ylab = "Total Within-Cluster Sum of Squares (WCSS)",
main = "Elbow Plot after PCA for Determining Optimal Number of Clusters")
abline(v = elbow_k, col = "red", lty = 2)  # Highlight the optimal K
set.seed(123)
k <- 10
# Apply PCA
pca_res <- PCA(as.matrix(tfidf), graph = FALSE)
library(FactoMineR)
# Apply PCA
pca_res <- PCA(as.matrix(tfidf), graph = FALSE)
# Apply PCA
pca_res <- PCA(as.matrix(tfidf), graph = FALSE)
# Scree plot
fviz_screeplot(pca_res, addlabels = TRUE)
# Biplot of the first two components
fviz_pca_biplot(pca_res, repel = TRUE)
# Analyze the top contributing words to the first few components
contrib <- get_pca_var(pca_res)$contrib
top_words_comp1 <- names(sort(contrib[,1], decreasing = TRUE)[1:10])
top_words_comp2 <- names(sort(contrib[,2], decreasing = TRUE)[1:10])
# Generate word clouds for the top components
wordcloud(words = top_words_comp1, max.words = 100)
wordcloud(words = top_words_comp2, max.words = 100)
# Perform K-means clustering on the PCA scores
pca_scores <- as.data.frame(pca_res$ind$coord)  # Get PCA scores for each document
set.seed(123)
km_res <- kmeans(pca_scores, centers = 10, nstart = 25)
# Visualize the PCA biplot with clusters
fviz_pca_biplot(pca_res, repel = TRUE, col.ind = as.factor(km_res$cluster)) +
scale_color_discrete(name = "Cluster") +
theme_minimal()
# Analyze the top contributing words for each cluster
for(i in 1:10) {
cluster_docs <- train_data[km_res$cluster == i]
wordcloud(cluster_docs, max.words = 100, random.order = FALSE, main = paste("Cluster", i))
}
cluster_docs <- train_corpus[km_res$cluster == i]
# Analyze the top contributing words for each cluster
for(i in 1:10) {
cluster_docs <- train_corpus[km_res$cluster == i]
wordcloud(cluster_docs, max.words = 100, random.order = FALSE, main = paste("Cluster", i))
}
# Perform PCA on the TF-IDF matrix
pca_res <- prcomp(tfidf_matrix, scale. = TRUE)
tfidf_matrix <- as.matrix(tfidf)
# Perform PCA on the TF-IDF matrix
pca_res <- prcomp(tfidf_matrix, scale. = TRUE)
# Perform PCA on the TF-IDF matrix
pca_res <- prcomp(tfidf_matrix, scale. = TRUE)
# Summary of PCA to understand the explained variance
summary(pca_res)
# Select the number of principal components that explain a significant amount of variance
explained_variance <- cumsum(pca_res$sdev^2) / sum(pca_res$sdev^2)
num_components <- which(explained_variance > 0.9)[1]  # Select components explaining 90% of the variance
# Reduce data to the selected number of principal components
pca_data <- pca_res$x[, 1:num_components]
# Summary of PCA to understand the explained variance
summary(pca_res)
# Perform K-means clustering on the PCA-transformed data
set.seed(123)
# Define a range of K values to test
k_values <- 1:10
wcss <- numeric(length(k_values))
# Perform K-means clustering for each K and store the within-cluster sum of squares (WCSS)
for (i in k_values) {
kmeans_res <- kmeans(pca_data, centers = i, nstart = 25)
wcss[i] <- kmeans_res$tot.withinss
}
# Plot the elbow plot
plot(k_values, wcss, type = "b", pch = 19, frame = FALSE,
xlab = "Number of clusters K", ylab = "Total within-cluster sum of squares (WCSS)")
# Identify the optimal K using the elbow method
best_k <- which(diff(diff(wcss)) > 0)[1] + 1
cat("The optimal number of clusters (K) according to the elbow method is:", best_k, "\n")
# Perform K-means clustering on the PCA-transformed data
set.seed(123)
k <- 2  # This can be adjusted or determined using methods like silhouette analysis
km_res <- kmeans(pca_data, centers = k, nstart = 25)
km_res <- kmeans(pca_data, centers = k, nstart = 25)
# Visualize clusters
fviz_cluster(km_res, data = pca_data, geom = "point", ellipse.type = "norm")
# Optionally, perform silhouette analysis to determine the optimal number of clusters
sil <- silhouette(km_res$cluster, dist(pca_data))
# Function to calculate WCSS for different values of K
wcss <- function(k) {
kmeans(tfidf, centers = k, nstart = 25)$tot.withinss
}
# Range of K values to try
k_values <- 1:10
# Calculate WCSS for each K
wcss_values <- map_dbl(k_values, wcss)
# Perform PCA on the TF-IDF matrix
pca_res <- PCA(as.data.frame(as.matrix(tfidf)), ncp = 50)  # Reduce to 50 components (you can adjust this number)
# Perform PCA on the TF-IDF matrix
pca_res <- PCA(as.data.frame(as.matrix(tfidf)), ncp = 50)  # Reduce to 50 components (you can adjust this number)
# Get the reduced data (principal components)
reduced_data <- pca_res$ind$coord
# Perform K-means clustering on the reduced data
wcss <- function(k) {
kmeans(reduced_data, centers = k, nstart = 25)$tot.withinss
}
# Now, you can create the elbow plot again using the reduced data
k_values <- 1:10
wcss_values <- map_dbl(k_values, wcss)
# Elbow plot
plot(k_values, wcss_values, type="b", pch = 19, frame = FALSE,
xlab="Number of clusters K",
ylab="Total within-clusters sum of squares",
main="Elbow Method for Optimal K (Reduced Dimensions)")
# Optimal K calculation remains the same
optimal_k <- which(diff(diff(wcss_values)) == min(diff(diff(wcss_values)))) + 1
cat("The optimal number of clusters (K) is:", optimal_k, "\n")
# Perform K-means clustering with K = 7
set.seed(123)  # Setting seed for reproducibility
k <- 7  # Optimal number of clusters
kmeans_res <- kmeans(reduced_data, centers = k, nstart = 25)
# Add the cluster assignment to the original data for further analysis
clustered_data <- as.data.frame(reduced_data)
clustered_data$cluster <- kmeans_res$cluster
# Visualize the clusters (using the first two principal components for 2D visualization)
library(ggplot2)
ggplot(clustered_data, aes(x = V1, y = V2, color = as.factor(cluster))) +
geom_point(size = 2) +
labs(title = "K-means Clustering on PCA-Reduced Data", x = "PC1", y = "PC2", color = "Cluster") +
theme_minimal()
# Visualize the clusters (using the first two principal components for 2D visualization)
ggplot(clustered_data, aes(x = V1, y = V2, color = as.factor(cluster))) +
geom_point(size = 2) +
labs(title = "K-means Clustering on PCA-Reduced Data", x = "PC1", y = "PC2", color = "Cluster") +
theme_minimal()
colnames(clustered_data)
# Visualize the clusters (using the first two principal components for 2D visualization)
ggplot(clustered_data, aes(x = Dim.1, y = Dim.2, color = as.factor(cluster))) +
geom_point(size = 2) +
labs(title = "K-means Clustering on PCA-Reduced Data", x = "PC1", y = "PC2", color = "Cluster") +
theme_minimal()
print(table(clustered_data$cluster))
# Summary of cluster sizes
cat("Cluster sizes:\n")
print(table(clustered_data$cluster))
# Optionally, examine the centroids of each cluster
cat("Cluster centroids:\n")
print(kmeans_res$centers)
# Assign original documents to clusters
cluster_assignments <- kmeans_res$cluster
dtm_clustered <- as.data.frame(as.matrix(dtm))
# Assign original documents to clusters
cluster_assignments <- kmeans_res$cluster
dtm_clustered <- as.data.frame(as.matrix(dtm))
dtm_clustered <- as.data.frame(as.matrix(dtm))
dtm_clustered$cluster <- cluster_assignments
# Find the top terms in each cluster
library(dplyr)
top_terms_by_cluster <- dtm_clustered %>%
group_by(cluster) %>%
summarise_all(mean) %>%
gather(term, frequency, -cluster) %>%
group_by(cluster) %>%
top_n(10, frequency)  # Get top 10 terms per cluster
print(top_terms_by_cluster)
print(top_terms_by_cluster)
# Step 1: Calculate the frequency of each cluster
cluster_frequencies <- table(clustered_data$cluster)
# Convert the table to a data frame for easier manipulation
cluster_freq_df <- as.data.frame(cluster_frequencies)
colnames(cluster_freq_df) <- c("cluster", "frequency")
# Step 2: Sort the clusters by frequency in descending order
cluster_freq_sorted <- cluster_freq_df %>%
arrange(desc(frequency))
# Print the sorted cluster frequencies
print(cluster_freq_sorted)
View(cluster_freq_sorted)
print(top_terms_by_cluster)
# Assuming your data is stored in a data frame called 'top_terms_by_cluster'
sorted_terms <- top_terms_by_cluster %>%
arrange(cluster, desc(frequency))  # Sort by cluster first, then by frequency in descending order
# Print the sorted table
print(sorted_terms)
# Create word clouds for each cluster
library(wordcloud)
for (i in unique(sorted_terms$cluster)) {
wordcloud(words = sorted_terms$term[sorted_terms$cluster == i],
freq = sorted_terms$frequency[sorted_terms$cluster == i],
scale = c(3, 0.5), max.words = 10, colors = brewer.pal(8, "Dark2"))
title(paste("Word Cloud for Cluster", i))
}
# Assuming 'sorted_terms' is the data frame with your sorted terms and clusters
summary_table <- sorted_terms %>%
group_by(cluster) %>%
summarise(
top_terms = paste(term, collapse = ", "),
top_frequencies = paste(round(frequency, 2), collapse = ", ")
)
# Print the summary table
print(summary_table)
# Print the sorted table
print(sorted_terms)
# Create word clouds for each cluster
library(wordcloud)
for (i in unique(sorted_terms$cluster)) {
wordcloud(words = sorted_terms$term[sorted_terms$cluster == i],
freq = sorted_terms$frequency[sorted_terms$cluster == i],
scale = c(3, 0.5), max.words = 10, colors = brewer.pal(8, "Dark2"))
title(paste("Word Cloud for Cluster", i))
}
# Print the sorted table
print(sorted_terms)
knitr::opts_chunk$set(echo = TRUE)
# Load libraries
library(tm)
library(SnowballC)
library(tidyverse)
library(cluster)
library(factoextra)
library(wordcloud)
library(FactoMineR)
library(dplyr)
print(table(clustered_data$cluster))
# Summary of cluster sizes
cat("Cluster sizes:\n")
print(table(clustered_data$cluster))
colnames(clustered_data)
cat("The optimal number of clusters (K) is:", optimal_k, "\n")
print(top_terms_by_cluster)
cat("The optimal number of clusters (K) is:", "\n", top_terms_by_cluster)
cat("The optimal number of clusters (K) is:", "\n")
cat("The optimal number of clusters (K) is:", "\n")
print(top_terms_by_cluster)
knitr::opts_chunk$set(echo = TRUE)
# Load libraries
library(tm)
library(SnowballC)
library(tidyverse)
library(cluster)
library(factoextra)
library(wordcloud)
library(FactoMineR)
library(dplyr)
library(wordcloud)
wordcloud(words = sorted_terms$term[sorted_terms$cluster == i],
freq = sorted_terms$frequency[sorted_terms$cluster == i],
scale = c(3, 0.5), max.words = 10, colors = brewer.pal(8, "Dark2"))
title(paste("Word Cloud for Cluster", i))
for (i in unique(sorted_terms$cluster)) {
wordcloud(words = sorted_terms$term[sorted_terms$cluster == i],
freq = sorted_terms$frequency[sorted_terms$cluster == i],
scale = c(3, 0.5), max.words = 10, colors = brewer.pal(8, "Dark2"))
title(paste("Word Cloud for Cluster", i))
}
print(sorted_terms, n = Inf)
# Load libraries
library(tidyverse)
library(arules)
library(arules)
library(arulesViz)
# Read the groceries data
groceries_raw <- readLines("groceries.txt")
# Read the groceries data
groceries_raw <- readLines("groceries.txt")
# Split each line by comma to create a list of baskets
groceries_list <- strsplit(groceries_raw, ",")
View(groceries_list)
View(groceries_list)
# Convert the list of baskets into a transactions object
groceries_trans <- as(groceries_list, "transactions")
View(groceries_trans)
# Summarize
summary(groceries_trans)
grocery_rules <- apriori(groceries_trans,
parameter = list(support = 0.01,
confidence = 0.2,
maxlen = 5))
View(grocery_rules)
View(groceries_list)
inspect(grocery_rules)
inspect(subset(grocery_rules, lift > 3 & confidence > 0.3))
inspect(grocery_rules)
inspect(subset(grocery_rules, lift > 3 & confidence > 0.3))
inspect(grocery_rules)
# Scatterplot of support vs confidence
plot(grocery_rules, method = "scatterplot", measure = c("support", "confidence"))
# Graph-based visualization
plot(grocery_rules, method = "graph", control = list(type = "items"))
inspect(grocery_rules)
# Two-key plot
plot(grocery_rules, method = "two-key plot")
# Scatterplot of support vs confidence
plot(grocery_rules, method = "scatterplot", measure = c("support", "confidence"))
# Graph-based visualization
plot(grocery_rules, method = "graph", control = list(type = "items"))
# Two-key plot
plot(grocery_rules, method = "two-key plot")
# Scatterplot of support vs confidence
plot(grocery_rules, method = "scatterplot", measure = c("support", "confidence"))
# Focus on strong associations and interesting rules
inspect(subset(grocery_rules, lift > 3 & confidence > 0.3))
# Focus on strong associations and interesting rules
inspect(subset(grocery_rules, lift > 3 & confidence > 0.3), n = Inf)
# Focus on strong associations and interesting rules
inspect(subset(grocery_rules, lift > 3 & confidence > 0.3))
inspect(grocery_rules, n = 10)
# Focus on strong associations and interesting rules
inspect(subset(grocery_rules, lift > 3 & confidence > 0.3))
# Scatterplot of support vs confidence
plot(grocery_rules, method = "scatterplot", measure = c("support", "confidence"))
# Graph-based visualization
plot(grocery_rules, method = "graph", control = list(type = "items"))
View(groceries_trans)
# Graph-based visualization
plot(grocery_rules, method = "graph", control = list(type = "items"))
# Summarize
summary(groceries_trans)
View(groceries_list)
View(groceries_trans)
# Graph-based visualization
plot(grocery_rules, method = "graph", control = list(type = "items"))
# Graph-based visualization
plot(grocery_rules, method = "graph", control = list(type = "items"))
strong_rules <- subset(grocery_rules, lift > 3 & confidence > 0.3)
plot(strong_rules, method = "graph", control = list(type = "items"))
# Summarize
summary(groceries_trans)
# Summarize
cat("Summary of the structure of the data after processing", "/n")
